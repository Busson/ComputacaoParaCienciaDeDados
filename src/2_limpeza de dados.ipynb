{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Limpeza, Normalização e Padronização de dados\n",
    "\n",
    "O pré-processamento de dados é uma etapa importante para o processo de análise de dados, pois a qualidade do resultado do seu modelo começa com a qualidade dos dados que você está “inputando”. Assim, parte considerável do tempo do cientista de dados é gasto no esforço que envolve a limpeza de dados e a engenharia de recursos (transformar dados brutos em atributos que melhor representem seus dados). Independentemente de o cientista de dados receber dados coletados ou ter que realizar a coleta, os dados estarão em formato bruto, que precisarão ser convertidos e filtrados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import datetime\n",
    "import numpy as np \n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Carregando o dataset\n",
    "\n",
    "O dataset da atividade anterior foi aumentado com novas colunas para simular caracteristicas de dados brutos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meu_data_frame = pd.read_pickle(\"../data/ugly_cereal.pkl\")\n",
    "display(meu_data_frame.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Limpeza de dados\n",
    "\n",
    "### 2.3.1 Eliminando atributos redundantes\n",
    "\n",
    "Neste exemplo, percebe-se que há redundancia entre as colunas \"data_cre_scorp\" e \"data_cre_seman\". Aparentemente o valor deste atributo em cada amostra é o mesmo, mas em formatos diferentes. Vamos checar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#flag que marca se as colunas são iguais\n",
    "sao_iguais = True\n",
    "\n",
    "#iterando sobre a coluna \"data_cre_saman\"\n",
    "for i in range(len(meu_data_frame[\"data_cre_saman\"].values)):\n",
    "    #convertendo os valores de \"data_cre_saman\" para \"data_cre_saman\", ex: Sunday 30. April 1995 -> 1195-4-30\n",
    "    alter_data = str(datetime.datetime.strptime(meu_data_frame[\"data_cre_saman\"].values[i], \"%A %d. %B %Y\")).replace(\" 00:00:00\",\"\")\n",
    "    comp_data = str(meu_data_frame[\"data_cre_scorp\"].values[i])\n",
    "    \n",
    "    #caso um único exemplo seja diferente, a flag é mercada como false\n",
    "    if comp_data != alter_data:\n",
    "            sao_iguais = False\n",
    "            break\n",
    "\n",
    "if sao_iguais:\n",
    "    print(\"Há redundância de informação, vou deletar a coluna 'data_cre_saman'\")\n",
    "    meu_data_frame.drop(columns=['data_cre_saman'], axis=1, inplace=True) \n",
    "else:\n",
    "    print(\"Não são iguais, é melhor pesquisar um pouco mais sobre a natureza desses atributos\")\n",
    "    \n",
    "display(meu_data_frame.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além das colunas redundantes, colunas com grande quantidade de dados nulos também devem ser removidas antes da filtragem por amostra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Removendo amostras com valores de atributos nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#um boa prática que antecede a remoção de valores nulos é a conversão de valores inválidos para nulos\n",
    "#por exemplo, campos com espaço em branco, caracteres especiais sem significado (?,*,.), etc.\n",
    "#Um regex pode ser usado para converter valores para NaN\n",
    "meu_data_frame = meu_data_frame.replace(r'^\\s*$', float(\"NaN\"), regex=True)\n",
    "\n",
    "#guardar as amostras irregulares é uma boa prática, lembre-se que esses dados podem ser revisados\n",
    "#e podem ser úteis no futuro.\n",
    "removed_data_frame = meu_data_frame[meu_data_frame.isnull().any(axis=1)].copy()\n",
    "\n",
    "#deletando as amostram que possuem algum valor NaN em qualquer atributo \n",
    "meu_data_frame = meu_data_frame.dropna()\n",
    "\n",
    "display(removed_data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Removendo amostras duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Dimensões do dataset (linha,coluna):\",  meu_data_frame.shape) \n",
    "meu_data_frame = meu_data_frame.drop_duplicates()\n",
    "print(\"Dimensões do dataset (linha,coluna) após eliminar duplicatas:\",  meu_data_frame.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 Remoção de símbolos especiais, escalas de medidas e grandezas numericas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#removendo o \"un.\" dos valores da coluna \"sales_week\" e convertendo de \"object\" para \"int64\"\n",
    "meu_data_frame[\"sales_week\"] = meu_data_frame[\"sales_week\"].str.replace(\"un.\",\"\")\n",
    "meu_data_frame[\"sales_week\"] = meu_data_frame[\"sales_week\"].astype('int64')\n",
    "display(meu_data_frame.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#removendo o cifrão dos valores da coluna \"price\" e convertendo de \"object\" para \"float64\"\n",
    "meu_data_frame[\"price\"] = meu_data_frame[\"price\"].str.replace(\"$\",\"\")\n",
    "meu_data_frame[\"price\"] = meu_data_frame[\"price\"].astype('float64')\n",
    "display(meu_data_frame.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5 Codificação de categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dados categoricos são convertidos para representação numérica (escala nominal)\n",
    "meu_data_frame[\"mfr\"] = meu_data_frame[\"mfr\"].cat.codes\n",
    "meu_data_frame[\"type\"] = meu_data_frame[\"type\"].cat.codes\n",
    "display(meu_data_frame.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Normalização e padronização\n",
    "\n",
    "A transformação dos seus dados, que já estão tratados, é uma pratica que tem vários impactos positivos na área de Ciência de Dados. Além de facilitar a visualizalção dos dados, a normalização e a padronização evitar que seu algoritmo  de aprendizado de máquina fique enviesado para as variáveis com maior ordem de grandeza.\n",
    "\n",
    "* A normalização tem como objetivo converter a distribuição original para uma distribuição dentro de um intervalo, por exemplo: [0,1] ou [-1,1]\n",
    "\n",
    "* A padronização tem como objetivo converter a distribuição original para uma distribuição com média 0 e desvio padrão 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instanciando o normalizador MinMAx\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "#obtendo as colunas do dataframe\n",
    "columns = meu_data_frame.columns\n",
    "\n",
    "#fazendo uma copia do dataframe, iremos normalizar a copia\n",
    "normalized_data_frame = meu_data_frame.copy()\n",
    "\n",
    "#iterando sobre cada coluna\n",
    "for column in columns:\n",
    "    #verificando se a coluna é numérica\n",
    "    if(meu_data_frame[column].dtype == \"int64\" or meu_data_frame[column].dtype == \"float64\"):\n",
    "        x = meu_data_frame[column].values\n",
    "        x_norm = min_max_scaler.fit_transform(x.reshape(-1, 1))\n",
    "        normalized_data_frame[column] = pd.DataFrame(x_norm)\n",
    "        normalized_data_frame.rename(columns={column:column+\"_norm\"}, inplace=True)\n",
    "\n",
    "display(normalized_data_frame.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "standard_scaler = preprocessing.StandardScaler()\n",
    "columns = meu_data_frame.columns\n",
    "\n",
    "standarlized_data_frame = meu_data_frame.copy()\n",
    "\n",
    "for column in columns:\n",
    "    if(meu_data_frame[column].dtype == \"int64\" or meu_data_frame[column].dtype == \"float64\"):\n",
    "        x = meu_data_frame[column].values\n",
    "        x_norm = standard_scaler.fit_transform(x.reshape(-1, 1))\n",
    "        standarlized_data_frame[column] = pd.DataFrame(x_norm)\n",
    "        standarlized_data_frame.rename(columns={column:column+\"_stda\"}, inplace=True)\n",
    "display(standarlized_data_frame.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Correlação de atributos\n",
    "\n",
    "O termo correlação representa, sob o ponto de vista da estatística, uma medida de associação\n",
    "entre duas ou mais variáveis. Por definição, se forem considerados numa população, os pares de valores de duas variáveis (xi;yi), a correlação pode ser definida pela equação de Pearson abaixo:\n",
    "\n",
    "<img src=\"imgs/corr.png\" width=35% />\n",
    "\n",
    "O valor da correção, conhecido como coeficiente de correlação, assume valores no intervalo de -1 a 1, de acordo com o grau de associação entre as variáveis em questão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculando a tabela de correlação\n",
    "corr = meu_data_frame.corr()\n",
    "\n",
    "p = 0.75 # correlação mínima\n",
    "var = []\n",
    "\n",
    "#iterando sobre a tabela\n",
    "for i in corr.columns:\n",
    "    for j in corr.columns:\n",
    "        if(i != j):\n",
    "            if np.abs(corr[i][j]) > p: # se maior do que |p|\n",
    "                var.append([i,j])\n",
    "print('Variáveis mais correlacionadas:\\n', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
